{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Word Embeddings from Scratch: The Complete Implementation\n",
        "\n",
        "**Author:** Prasenjit Giri  \n",
        "**Date:** January 2023  \n",
        "**Blog:** [Word Embeddings from Scratch: The Journey from Words to Vectors](https://prasenjitgiri.github.io/post.html?id=word-embeddings-from-scratch)\n",
        "\n",
        "This notebook contains the complete implementation of word embeddings from scratch, demonstrating how models like ChatGPT process individual words into numerical representations.\n",
        "\n",
        "## Core Question: 20 Words = 20 Embeddings or 1?\n",
        "\n",
        "**Answer: 20 individual word embeddings, one for each word.**\n",
        "\n",
        "This notebook proves this mathematically and shows exactly how it works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required dependencies\n",
        "import numpy as np\n",
        "import re\n",
        "import logging\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Tuple, Dict, Set, Optional\n",
        "import pickle\n",
        "import random\n",
        "import json\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"‚úì All dependencies imported successfully\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "\n",
        "# Set up matplotlib for inline plotting\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete WordEmbeddingTrainer implementation\n",
        "class WordEmbeddingTrainer:\n",
        "    \"\"\"\n",
        "    Skip-Gram word embedding trainer with negative sampling.\n",
        "    \n",
        "    Key insight: Each word gets its own embedding vector.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embedding_dim=100, window_size=3, negative_samples=5, \n",
        "                 learning_rate=0.01, min_count=2, epochs=20):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.window_size = window_size\n",
        "        self.negative_samples = negative_samples\n",
        "        self.learning_rate = learning_rate\n",
        "        self.min_count = min_count\n",
        "        self.epochs = epochs\n",
        "        \n",
        "        # Vocabulary\n",
        "        self.word_to_idx = {}\n",
        "        self.idx_to_word = {}\n",
        "        self.word_counts = Counter()\n",
        "        self.vocab_size = 0\n",
        "        \n",
        "        # Embedding matrices\n",
        "        self.W1 = None  # Input embeddings\n",
        "        self.W2 = None  # Output embeddings\n",
        "        self.training_pairs = []\n",
        "    \n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Clean and tokenize text.\"\"\"\n",
        "        text = text.lower().strip()\n",
        "        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:]', '', text)\n",
        "        return [token for token in text.split() if len(token) > 1]\n",
        "    \n",
        "    def build_vocabulary(self, texts):\n",
        "        \"\"\"Build vocabulary with frequency filtering.\"\"\"\n",
        "        print(\"Building vocabulary...\")\n",
        "        for text in texts:\n",
        "            tokens = self.preprocess_text(text)\n",
        "            self.word_counts.update(tokens)\n",
        "        \n",
        "        filtered_words = {word: count for word, count in self.word_counts.items() \n",
        "                         if count >= self.min_count}\n",
        "        \n",
        "        self.word_to_idx = {word: idx for idx, word in enumerate(filtered_words.keys())}\n",
        "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
        "        self.vocab_size = len(self.word_to_idx)\n",
        "        \n",
        "        print(f\"‚úì Vocabulary: {self.vocab_size} words\")\n",
        "    \n",
        "    def generate_training_data(self, texts):\n",
        "        \"\"\"Generate Skip-Gram training pairs.\"\"\"\n",
        "        print(\"Generating training pairs...\")\n",
        "        self.training_pairs = []\n",
        "        \n",
        "        for text in texts:\n",
        "            tokens = self.preprocess_text(text)\n",
        "            indices = [self.word_to_idx[token] for token in tokens if token in self.word_to_idx]\n",
        "            \n",
        "            for i, target_idx in enumerate(indices):\n",
        "                start = max(0, i - self.window_size)\n",
        "                end = min(len(indices), i + self.window_size + 1)\n",
        "                \n",
        "                for j in range(start, end):\n",
        "                    if i != j:\n",
        "                        context_idx = indices[j]\n",
        "                        self.training_pairs.append((target_idx, context_idx))\n",
        "        \n",
        "        print(f\"‚úì Generated {len(self.training_pairs)} training pairs\")\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Stable sigmoid function.\"\"\"\n",
        "        return np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
        "    \n",
        "    def negative_sampling(self, target_idx, positive_context):\n",
        "        \"\"\"Sample negative examples.\"\"\"\n",
        "        if not hasattr(self, '_neg_probs'):\n",
        "            word_freqs = np.array([self.word_counts[self.idx_to_word[i]] for i in range(self.vocab_size)])\n",
        "            word_freqs = np.power(word_freqs, 0.75)\n",
        "            self._neg_probs = word_freqs / np.sum(word_freqs)\n",
        "        \n",
        "        negative_samples = []\n",
        "        attempts = 0\n",
        "        while len(negative_samples) < self.negative_samples and attempts < 50:\n",
        "            candidate = np.random.choice(self.vocab_size, p=self._neg_probs)\n",
        "            if candidate != target_idx and candidate != positive_context:\n",
        "                negative_samples.append(candidate)\n",
        "            attempts += 1\n",
        "        \n",
        "        return negative_samples\n",
        "    \n",
        "    def train_step(self, target_idx, context_idx):\n",
        "        \"\"\"One training step with negative sampling.\"\"\"\n",
        "        target_embedding = self.W1[target_idx].copy()\n",
        "        \n",
        "        # Positive sample\n",
        "        positive_score = np.dot(target_embedding, self.W2[context_idx])\n",
        "        positive_prob = self.sigmoid(positive_score)\n",
        "        positive_loss = -np.log(positive_prob + 1e-10)\n",
        "        \n",
        "        positive_error = positive_prob - 1\n",
        "        context_grad = positive_error * target_embedding\n",
        "        target_grad = positive_error * self.W2[context_idx]\n",
        "        \n",
        "        # Negative samples\n",
        "        negative_samples = self.negative_sampling(target_idx, context_idx)\n",
        "        negative_loss = 0\n",
        "        \n",
        "        for neg_idx in negative_samples:\n",
        "            negative_score = np.dot(target_embedding, self.W2[neg_idx])\n",
        "            negative_prob = self.sigmoid(-negative_score)\n",
        "            negative_loss += -np.log(negative_prob + 1e-10)\n",
        "            \n",
        "            negative_error = -(1 - negative_prob)\n",
        "            self.W2[neg_idx] += self.learning_rate * negative_error * target_embedding\n",
        "            target_grad += negative_error * self.W2[neg_idx]\n",
        "        \n",
        "        # Update embeddings\n",
        "        self.W2[context_idx] += self.learning_rate * context_grad\n",
        "        self.W1[target_idx] += self.learning_rate * target_grad\n",
        "        \n",
        "        return positive_loss + negative_loss\n",
        "    \n",
        "    def train(self, texts):\n",
        "        \"\"\"Train the model.\"\"\"\n",
        "        self.build_vocabulary(texts)\n",
        "        self.generate_training_data(texts)\n",
        "        \n",
        "        # Initialize embeddings (Xavier initialization)\n",
        "        std = np.sqrt(2.0 / (self.vocab_size + self.embedding_dim))\n",
        "        self.W1 = np.random.normal(0, std, (self.vocab_size, self.embedding_dim))\n",
        "        self.W2 = np.random.normal(0, std, (self.vocab_size, self.embedding_dim))\n",
        "        \n",
        "        print(f\"Training for {self.epochs} epochs...\")\n",
        "        epoch_losses = []\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            epoch_loss = 0\n",
        "            random.shuffle(self.training_pairs)\n",
        "            \n",
        "            for i, (target_idx, context_idx) in enumerate(self.training_pairs):\n",
        "                loss = self.train_step(target_idx, context_idx)\n",
        "                epoch_loss += loss\n",
        "                \n",
        "                if i % 2000 == 0 and i > 0:\n",
        "                    print(f\"Epoch {epoch+1}/{self.epochs}, Step {i}, Avg Loss: {epoch_loss/(i+1):.4f}\")\n",
        "            \n",
        "            avg_loss = epoch_loss / len(self.training_pairs)\n",
        "            epoch_losses.append(avg_loss)\n",
        "            self.learning_rate *= 0.95\n",
        "            print(f\"‚úì Epoch {epoch+1} completed, Loss: {avg_loss:.4f}\")\n",
        "        \n",
        "        return {'epoch_losses': epoch_losses, 'vocab_size': self.vocab_size}\n",
        "    \n",
        "    def get_word_vector(self, word):\n",
        "        \"\"\"Get embedding for a word.\"\"\"\n",
        "        return self.W1[self.word_to_idx[word]] if word in self.word_to_idx else None\n",
        "    \n",
        "    def cosine_similarity(self, word1, word2):\n",
        "        \"\"\"Calculate cosine similarity between two words.\"\"\"\n",
        "        vec1, vec2 = self.get_word_vector(word1), self.get_word_vector(word2)\n",
        "        if vec1 is None or vec2 is None:\n",
        "            return 0.0\n",
        "        \n",
        "        dot_product = np.dot(vec1, vec2)\n",
        "        norm1, norm2 = np.linalg.norm(vec1), np.linalg.norm(vec2)\n",
        "        return dot_product / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0.0\n",
        "    \n",
        "    def find_similar_words(self, word, top_k=3):\n",
        "        \"\"\"Find most similar words.\"\"\"\n",
        "        if self.get_word_vector(word) is None:\n",
        "            return []\n",
        "        \n",
        "        similarities = [(w, self.cosine_similarity(word, w)) \n",
        "                       for w in self.word_to_idx.keys() if w != word]\n",
        "        return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "print(\"‚úì WordEmbeddingTrainer class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training corpus - diverse sentences for learning\n",
        "training_corpus = [\n",
        "    \"The cat sat on the mat and looked around the room carefully\",\n",
        "    \"A dog ran quickly through the park and played with children happily\", \n",
        "    \"Machine learning algorithms process data efficiently and accurately every time\",\n",
        "    \"Neural networks learn complex patterns from examples and training data sets\",\n",
        "    \"Natural language processing enables computer understanding of human text documents\",\n",
        "    \"Deep learning models require large amounts of training data to work well\",\n",
        "    \"The quick brown fox jumps over the lazy dog in the field\",\n",
        "    \"Artificial intelligence will transform many industries in the coming years\",\n",
        "    \"Python programming language is popular for data science and machine learning\",\n",
        "    \"Word embeddings capture semantic relationships between words and concepts effectively\",\n",
        "    \"ChatGPT represents a breakthrough in conversational artificial intelligence systems\",\n",
        "    \"Language models use attention mechanisms to understand context and meaning\",\n",
        "    \"Transformers have revolutionized natural language processing applications worldwide\",\n",
        "    \"Researchers continue advancing the field of artificial intelligence rapidly\",\n",
        "    \"Data scientists use various tools and techniques for analysis and modeling\",\n",
        "    \"Computer vision systems can recognize objects and faces in images\",\n",
        "    \"Robotics combines mechanical engineering with artificial intelligence\",\n",
        "    \"Cloud computing provides scalable infrastructure for machine learning\",\n",
        "    \"Software engineers develop applications using modern programming languages\",\n",
        "    \"Big data analytics helps companies make better business decisions\"\n",
        "]\n",
        "\n",
        "print(f\"üìö Training corpus: {len(training_corpus)} sentences\")\n",
        "print(f\"üìù Sample: {training_corpus[0]}\")\n",
        "\n",
        "# Count words\n",
        "total_words = sum(len(sentence.split()) for sentence in training_corpus)\n",
        "all_words = []\n",
        "for sentence in training_corpus:\n",
        "    all_words.extend(sentence.lower().split())\n",
        "unique_words = len(set(all_words))\n",
        "\n",
        "print(f\"üìä Total words: {total_words}\")\n",
        "print(f\"üìä Unique words: {unique_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer = WordEmbeddingTrainer(\n",
        "    embedding_dim=100,  # 100-dimensional embeddings\n",
        "    window_size=3,      # Context window of 3 words each side\n",
        "    negative_samples=5, # 5 negative samples per positive\n",
        "    learning_rate=0.01, # Conservative learning rate\n",
        "    min_count=2,        # Include words appearing 2+ times\n",
        "    epochs=25           # 25 training epochs\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(\"This will take a few moments...\")\n",
        "\n",
        "# Train the model\n",
        "metrics = trainer.train(training_corpus)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéâ TRAINING COMPLETED!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"‚úÖ Final vocabulary size: {metrics['vocab_size']}\")\n",
        "print(f\"‚úÖ Final loss: {metrics['epoch_losses'][-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# THE CORE DEMONSTRATION: 20 Words = 20 Individual Embeddings\n",
        "def demonstrate_individual_embeddings(trainer, sentence):\n",
        "    \"\"\"\n",
        "    This function proves our core claim: each word gets its own embedding.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéØ CORE DEMONSTRATION: EACH WORD GETS ITS OWN EMBEDDING\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    words = sentence.lower().split()\n",
        "    \n",
        "    print(f\"üìù Sentence: '{sentence}'\")\n",
        "    print(f\"üìä Number of words: {len(words)}\")\n",
        "    print(f\"üìã Words: {words}\")\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    \n",
        "    embeddings = []\n",
        "    for i, word in enumerate(words):\n",
        "        embedding = trainer.get_word_vector(word)\n",
        "        if embedding is not None:\n",
        "            embeddings.append(embedding)\n",
        "            print(f\"Word {i+1:2d}: '{word:12s}' ‚Üí {embedding.shape} vector | First 3: [{embedding[0]:.3f}, {embedding[1]:.3f}, {embedding[2]:.3f}]\")\n",
        "        else:\n",
        "            print(f\"Word {i+1:2d}: '{word:12s}' ‚Üí [NOT IN VOCABULARY]\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(f\"üèÜ RESULT: {len(words)} words = {len(embeddings)} individual embeddings\")\n",
        "    print(\"‚úÖ Each word maintains its unique vector representation!\")\n",
        "    \n",
        "    # Show sentence-level combination\n",
        "    if embeddings:\n",
        "        sentence_embedding = np.mean(embeddings, axis=0)\n",
        "        print(f\"\\nüìà Sentence embedding (mean pooling): {sentence_embedding.shape}\")\n",
        "        print(f\"üìà First 5 dims: [{', '.join([f'{x:.3f}' for x in sentence_embedding[:5]])}]\")\n",
        "        \n",
        "        print(\"\\nüí° Methods to combine word embeddings:\")\n",
        "        print(\"   1. ‚úÖ Mean pooling (averaging) - what we just did\")\n",
        "        print(\"   2. Weighted averaging (TF-IDF, attention weights)\")\n",
        "        print(\"   3. Max pooling (element-wise maximum)\")\n",
        "        print(\"   4. LSTM/GRU encoders\")\n",
        "        print(\"   5. Transformer attention (ChatGPT approach)\")\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "# Test with exactly 20 words\n",
        "test_sentence = \"Machine learning algorithms process data efficiently and neural networks learn complex patterns from training examples in artificial intelligence systems\"\n",
        "word_count = len(test_sentence.split())\n",
        "print(f\"üéØ Testing with {word_count}-word sentence...\")\n",
        "\n",
        "embeddings = demonstrate_individual_embeddings(trainer, test_sentence)\n",
        "\n",
        "print(f\"\\nüéâ CONCLUSION: {word_count} words = {len(embeddings)} individual embeddings\")\n",
        "print(\"üîë This is exactly how ChatGPT processes text!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
